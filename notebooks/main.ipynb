{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando a estrutura do código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, QuantileTransformer, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajustando os paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "data_path = str(Path(os.getcwd()).parent / \"data\")\n",
    "artifacts_path = str(Path(os.getcwd()).parent / \"artifacts\")\n",
    "logs_path = str(Path(os.getcwd()).parent / \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para registrar os logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configuração do logger\n",
    "LOG_DUMP_PATH = f'{logs_path}/nt_failure.log'\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=LOG_DUMP_PATH,\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "def _log_failure(e):\n",
    "    logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Carrega o dataset em formato Parquet.\"\"\"\n",
    "    try:\n",
    "        return pd.read_parquet(file_path)\n",
    "    except Exception as e:\n",
    "        _log_failure(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_path: str):\n",
    "    \"\"\"Carrega o modelo treinado salvo em pickle.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except Exception as e:\n",
    "        _log_failure(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregamento da pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline(file_path: str) -> Pipeline:\n",
    "    try:\n",
    "        # Carrega o arquivo JSONC\n",
    "        with open(file_path, 'r') as f:\n",
    "            str_json = '\\n'.join(f.readlines()[3:])\n",
    "            pipeline_config = json.loads(str_json)\n",
    "\n",
    "        # Configura as etapas do pipeline\n",
    "        steps = []\n",
    "\n",
    "        if \"reduce_dim\" in pipeline_config[\"steps\"]:\n",
    "            steps.append((\"reduce_dim\", PolynomialFeatures(**pipeline_config[\"steps\"][\"reduce_dim\"][\"PolynomialFeatures\"])))\n",
    "\n",
    "        if \"qtransf\" in pipeline_config[\"steps\"]:\n",
    "            steps.append((\"qtransf\", QuantileTransformer(**pipeline_config[\"steps\"][\"qtransf\"][\"QuantileTransformer\"])))\n",
    "\n",
    "        if \"poly_feature\" in pipeline_config[\"steps\"]:\n",
    "            steps.append((\"poly_feature\", PolynomialFeatures(**pipeline_config[\"steps\"][\"poly_feature\"][\"PolynomialFeatures\"])))\n",
    "\n",
    "        if \"stdscaler\" in pipeline_config[\"steps\"]:\n",
    "            steps.append((\"stdscaler\", StandardScaler(**pipeline_config[\"steps\"][\"stdscaler\"][\"StandardScaler\"])))\n",
    "\n",
    "        # Retorna o pipeline\n",
    "        return Pipeline(steps)\n",
    "    except Exception as e:\n",
    "        _log_failure(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(data_path: str, model_path: str, pipeline_path: str):\n",
    "    try:\n",
    "        # Carregar o modelo, os dados e o pipeline\n",
    "        data = load_data(data_path)\n",
    "        model = load_model(model_path)\n",
    "        pipeline = load_pipeline(pipeline_path)\n",
    "\n",
    "        # Seleciona as colunas de interesse\n",
    "        data = data[['vibration_x', 'vibration_y', 'vibration_z']]\n",
    "\n",
    "        # Ajusta o pipeline com os dados\n",
    "        pipeline.fit(data)\n",
    "        transformed_data = pipeline.transform(data)\n",
    "\n",
    "        if not len(transformed_data):\n",
    "            raise RuntimeError('No data to score')\n",
    "        if not hasattr(model, 'predict'):\n",
    "            raise Exception('Model does not have a predict function')\n",
    "\n",
    "        # Retorna a previsão do modelo\n",
    "        return model.predict(transformed_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        _log_failure(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução da pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1  1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      " -1  1 -1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1 -1  1  1  1 -1  1  1  1  1 -1  1 -1  1  1  1\n",
      " -1 -1 -1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1 -1  1 -1  1 -1 -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ernan\\anaconda3\\envs\\venv-shape-challenge\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2590: UserWarning: n_quantiles (1000) is greater than the total number of samples (295). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Caminhos dos arquivos\n",
    "data_path = f'{data_path}/dataset.parquet'\n",
    "model_path = f'{artifacts_path}/model.pkl'\n",
    "pipeline_path = f'{artifacts_path}/pipeline.jsonc'\n",
    "\n",
    "# Executar a função de scoring\n",
    "result = score(data_path, model_path, pipeline_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-shape-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
